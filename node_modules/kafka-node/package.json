{
  "name": "kafka-node",
  "description": "Client for Apache Kafka v0.8+",
  "keywords": [
    "kafka",
    "zookeeper",
    "consumer",
    "producer",
    "broker"
  ],
  "bugs": {
    "url": "https://github.com/SOHU-co/kafka-node/issues"
  },
  "version": "0.5.7",
  "main": "kafka.js",
  "license": "MIT",
  "dependencies": {
    "async": ">0.9 <2.0",
    "binary": "~0.3.0",
    "buffer-crc32": "~0.2.5",
    "buffermaker": "~1.2.0",
    "debug": "^2.1.3",
    "lodash": ">3.0 <4.0",
    "minimatch": "^3.0.2",
    "nested-error-stacks": "^1.0.2",
    "node-uuid": "~1.4.3",
    "node-zookeeper-client": "~0.2.2",
    "optional": "^0.1.3",
    "retry": "~0.6.1",
    "snappy": "^5.0.5"
  },
  "optionalDependencies": {
    "snappy": "^5.0.5"
  },
  "devDependencies": {
    "coveralls": "^2.11.12",
    "doctoc": "^1.2.0",
    "eslint": "^2.13.1",
    "eslint-config-semistandard": "^6.0.2",
    "eslint-config-standard": "^5.3.1",
    "eslint-plugin-promise": ">=1.0.8",
    "eslint-plugin-standard": ">=1.3.1",
    "istanbul": "^0.4.4",
    "mocha": "^2.2.1",
    "optimist": "^0.6.1",
    "proxyquire": "^1.7.10",
    "should": "^6.0.0",
    "sinon": "^1.17.2"
  },
  "repository": {
    "type": "git",
    "url": "git@github.com:SOHU-Co/kafka-node.git"
  },
  "scripts": {
    "test": "eslint . && ./run-tests.sh",
    "startDocker": "./start-docker.sh",
    "stopDocker": "docker-compose down",
    "updateToc": "doctoc README.md --maxlevel 2 --notitle"
  },
  "readme": "Kafka-node\n==========\n\n[![NPM](https://nodei.co/npm/kafka-node.png)](https://nodei.co/npm/kafka-node/)\n[![NPM](https://nodei.co/npm-dl/kafka-node.png?height=3)](https://nodei.co/npm/kafka-node/)\n[![Build Status](https://travis-ci.org/SOHU-Co/kafka-node.svg?branch=master)](https://travis-ci.org/SOHU-Co/kafka-node)\n[![Coverage Status](https://coveralls.io/repos/github/SOHU-Co/kafka-node/badge.svg?branch=master)](https://coveralls.io/github/SOHU-Co/kafka-node?branch=master)\n\nKafka-node is a Node.js client with Zookeeper integration for Apache Kafka 0.8.1 and later.\n\n# Table of Contents\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n\n- [Features](#features)\n- [Install Kafka](#install-kafka)\n- [API](#api)\n  - [Client](#client)\n  - [Producer](#producer)\n  - [HighLevelProducer](#highlevelproducer)\n  - [Consumer](#consumer)\n  - [HighLevelConsumer](#highlevelconsumer)\n  - [Offset](#offset)\n- [Troubleshooting / FAQ](#troubleshooting--faq)\n  - [HighLevelProducer with KeyedPartitioner errors on first send](#highlevelproducer-with-keyedpartitioner-errors-on-first-send)\n  - [How do I debug an issue?](#how-do-i-debug-an-issue)\n  - [For a new consumer how do I start consuming from the latest message in a partition?](#for-a-new-consumer-how-do-i-start-consuming-from-the-latest-message-in-a-partition)\n  - [FailedToRebalanceConsumerError: Exception: NODE_EXISTS[-110]](#failedtorebalanceconsumererror-exception-node_exists-110)\n  - [HighLevelConsumer does not consume on all partitions](#highlevelconsumer-does-not-consume-on-all-partitions)\n  - [How to throttle messages / control the concurrency of processing messages](#how-to-throttle-messages--control-the-concurrency-of-processing-messages)\n- [Running Tests](#running-tests)\n- [LICENSE - \"MIT\"](#license---mit)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n# Features\n* Consumer and High Level Consumer\n* Producer and High Level Producer\n* Manage topic Offsets\n* SSL connections to brokers (Kafka 0.9+)\n\n# Install Kafka\nFollow the [instructions](http://kafka.apache.org/documentation.html#quickstart) on the Kafka wiki to build Kafka 0.8 and get a test broker up and running.\n\n# API\n## Client\n### Client(connectionString, clientId, [zkOptions], [noAckBatchOptions], [sslOptions])\n* `connectionString`: Zookeeper connection string, default `localhost:2181/`\n* `clientId`: This is a user-supplied identifier for the client application, default `kafka-node-client`\n* `zkOptions`: **Object**, Zookeeper options, see [node-zookeeper-client](https://github.com/alexguan/node-zookeeper-client#client-createclientconnectionstring-options)\n* `noAckBatchOptions`: **Object**, when requireAcks is disabled on Producer side we can define the batch properties, 'noAckBatchSize' in bytes and 'noAckBatchAge' in milliseconds. The default value is `{ noAckBatchSize: null, noAckBatchAge: null }` and it acts as if there was no batch\n* `sslOptions`: **Object**, options to be passed to the tls broker sockets, ex. { rejectUnauthorized: false } (Kafka +0.9)\n\n### close(cb)\nCloses the connection to Zookeeper and the brokers so that the node process can exit gracefully.\n\n* `cb`: **Function**, the callback\n\n## Producer\n### Producer(client, [options])\n* `client`: client which keeps a connection with the Kafka server.\n* `options`: options for producer,\n\n```js\n{\n    // Configuration for when to consider a message as acknowledged, default 1\n    requireAcks: 1,\n    // The amount of time in milliseconds to wait for all acks before considered, default 100ms\n    ackTimeoutMs: 100,\n    // Partitioner type (default = 0, random = 1, cyclic = 2, keyed = 3), default 0\n    partitionerType: 2\n}\n```\n\n``` js\nvar kafka = require('kafka-node'),\n    Producer = kafka.Producer,\n    client = new kafka.Client(),\n    producer = new Producer(client);\n```\n\n### Events\n\n- `ready`: this event is emitted when producer is ready to send messages.\n- `error`: this is the error event propagates from internal client, producer should always listen it.\n\n### send(payloads, cb)\n* `payloads`: **Array**,array of `ProduceRequest`, `ProduceRequest` is a JSON object like:\n\n``` js\n{\n   topic: 'topicName',\n   messages: ['message body'], // multi messages should be a array, single message can be just a string or a KeyedMessage instance\n   key: 'theKey', // only needed when using keyed partitioner\n   partition: 0, // default 0\n   attributes: 2 // default: 0\n}\n```\n\n* `cb`: **Function**, the callback\n\n`attributes` controls compression of the message set. It supports the following values:\n\n* `0`: No compression\n* `1`: Compress using GZip\n* `2`: Compress using snappy\n\nExample:\n\n```js\nvar kafka = require('kafka-node'),\n    Producer = kafka.Producer,\n    KeyedMessage = kafka.KeyedMessage,\n    client = new kafka.Client(),\n    producer = new Producer(client),\n    km = new KeyedMessage('key', 'message'),\n    payloads = [\n        { topic: 'topic1', messages: 'hi', partition: 0 },\n        { topic: 'topic2', messages: ['hello', 'world', km] }\n    ];\nproducer.on('ready', function () {\n    producer.send(payloads, function (err, data) {\n        console.log(data);\n    });\n});\n\nproducer.on('error', function (err) {})\n```\n> ⚠️**WARNING**: Batch multiple messages of the same topic/partition together as an array on the `messages` attribute otherwise you may lose messages!\n\n### createTopics(topics, async, cb)\nThis method is used to create topics on the Kafka server. It only works when `auto.create.topics.enable`, on the Kafka server, is set to true. Our client simply sends a metadata request to the server which will auto create topics. When `async` is set to false, this method does not return until all topics are created, otherwise it returns immediately.\n\n* `topics`: **Array**, array of topics\n* `async`: **Boolean**, async or sync\n* `cb`: **Function**, the callback\n\nExample:\n\n``` js\nvar kafka = require('kafka-node'),\n    Producer = kafka.Producer,\n    client = new kafka.Client(),\n    producer = new Producer(client);\n// Create topics sync\nproducer.createTopics(['t','t1'], false, function (err, data) {\n    console.log(data);\n});\n// Create topics async\nproducer.createTopics(['t'], true, function (err, data) {});\nproducer.createTopics(['t'], function (err, data) {});// Simply omit 2nd arg\n```\n\n\n## HighLevelProducer\n### HighLevelProducer(client, [options])\n* `client`: client which keeps a connection with the Kafka server. Round-robins produce requests to the available topic partitions\n* `options`: options for producer,\n\n```js\n{\n    // Configuration for when to consider a message as acknowledged, default 1\n    requireAcks: 1,\n    // The amount of time in milliseconds to wait for all acks before considered, default 100ms\n    ackTimeoutMs: 100,\n    // Partitioner type (default = 0, random = 1, cyclic = 2, keyed = 3), default 2\n    partitionerType: 3\n}\n```\n\n``` js\nvar kafka = require('kafka-node'),\n    HighLevelProducer = kafka.HighLevelProducer,\n    client = new kafka.Client(),\n    producer = new HighLevelProducer(client);\n```\n\n### Events\n\n- `ready`: this event is emitted when producer is ready to send messages.\n- `error`: this is the error event propagates from internal client, producer should always listen it.\n\n### send(payloads, cb)\n* `payloads`: **Array**,array of `ProduceRequest`, `ProduceRequest` is a JSON object like:\n\n``` js\n{\n   topic: 'topicName',\n   messages: ['message body'], // multi messages should be a array, single message can be just a string,\n   key: 'theKey', // only needed when using keyed partitioner\n   attributes: 1\n}\n```\n\n* `cb`: **Function**, the callback\n\nExample:\n\n``` js\nvar kafka = require('kafka-node'),\n    HighLevelProducer = kafka.HighLevelProducer,\n    client = new kafka.Client(),\n    producer = new HighLevelProducer(client),\n    payloads = [\n        { topic: 'topic1', messages: 'hi' },\n        { topic: 'topic2', messages: ['hello', 'world'] }\n    ];\nproducer.on('ready', function () {\n    producer.send(payloads, function (err, data) {\n        console.log(data);\n    });\n});\n```\n> ⚠️**WARNING**: Batch multiple messages of the same topic/partition together as an array on the `messages` attribute otherwise you may lose messages!\n\n### createTopics(topics, async, cb)\nThis method is used to create topics on the Kafka server. It only work when `auto.create.topics.enable`, on the Kafka server, is set to true. Our client simply sends a metadata request to the server which will auto create topics. When `async` is set to false, this method does not return until all topics are created, otherwise it returns immediately.\n\n* `topics`: **Array**,array of topics\n* `async`: **Boolean**,async or sync\n* `cb`: **Function**,the callback\n\nExample:\n\n``` js\nvar kafka = require('kafka-node'),\n    HighLevelProducer = kafka.HighLevelProducer,\n    client = new kafka.Client(),\n    producer = new HighLevelProducer(client);\n// Create topics sync\nproducer.createTopics(['t','t1'], false, function (err, data) {\n    console.log(data);\n});\n// Create topics async\nproducer.createTopics(['t'], true, function (err, data) {});\nproducer.createTopics(['t'], function (err, data) {});// Simply omit 2nd arg\n```\n\n## Consumer\n### Consumer(client, payloads, options)\n* `client`: client which keeps a connection with the Kafka server. **Note**: it's recommend that create new client for different consumers.\n* `payloads`: **Array**,array of `FetchRequest`, `FetchRequest` is a JSON object like:\n\n``` js\n{\n   topic: 'topicName',\n   offset: 0, //default 0\n}\n```\n\n* `options`: options for consumer,\n\n```js\n{\n    groupId: 'kafka-node-group',//consumer group id, default `kafka-node-group`\n    // Auto commit config\n    autoCommit: true,\n    autoCommitIntervalMs: 5000,\n    // The max wait time is the maximum amount of time in milliseconds to block waiting if insufficient data is available at the time the request is issued, default 100ms\n    fetchMaxWaitMs: 100,\n    // This is the minimum number of bytes of messages that must be available to give a response, default 1 byte\n    fetchMinBytes: 1,\n    // The maximum bytes to include in the message set for this partition. This helps bound the size of the response.\n    fetchMaxBytes: 1024 * 1024,\n    // If set true, consumer will fetch message from the given offset in the payloads\n    fromOffset: false,\n    // If set to 'buffer', values will be returned as raw buffer objects.\n    encoding: 'utf8'\n}\n```\nExample:\n\n``` js\nvar kafka = require('kafka-node'),\n    Consumer = kafka.Consumer,\n    client = new kafka.Client(),\n    consumer = new Consumer(\n        client,\n        [\n            { topic: 't', partition: 0 }, { topic: 't1', partition: 1 }\n        ],\n        {\n            autoCommit: false\n        }\n    );\n```\n\n### on('message', onMessage);\nBy default, we will consume messages from the last committed offset of the current group\n\n* `onMessage`: **Function**, callback when new message comes\n\nExample:\n\n``` js\nconsumer.on('message', function (message) {\n    console.log(message);\n});\n```\n\n### on('error', function (err) {})\n\n\n### on('offsetOutOfRange', function (err) {})\n\n\n### addTopics(topics, cb, fromOffset)\nAdd topics to current consumer, if any topic to be added not exists, return error\n* `topics`: **Array**, array of topics to add\n* `cb`: **Function**,the callback\n* `fromOffset`: **Boolean**, if true, the consumer will fetch message from the specified offset, otherwise it will fetch message from the last commited offset of the topic.\n\nExample:\n\n``` js\nconsumer.addTopics(['t1', 't2'], function (err, added) {\n});\n\nor\n\nconsumer.addTopics([{ topic: 't1', offset: 10 }], function (err, added) {\n}, true);\n```\n\n### removeTopics(topics, cb)\n* `topics`: **Array**, array of topics to remove\n* `cb`: **Function**, the callback\n\nExample:\n\n``` js\nconsumer.removeTopics(['t1', 't2'], function (err, removed) {\n});\n```\n\n### commit(cb)\nCommit offset of the current topics manually, this method should be called when a consumer leaves\n\n* `cb`: **Function**, the callback\n\nExample:\n\n``` js\nconsumer.commit(function(err, data) {\n});\n```\n\n### setOffset(topic, partition, offset)\nSet offset of the given topic\n\n* `topic`: **String**\n\n* `partition`: **Number**\n\n* `offset`: **Number**\n\nExample:\n\n``` js\nconsumer.setOffset('topic', 0, 0);\n```\n\n### pause()\nPause the consumer. ***Calling `pause` does not automatically stop messages from being emitted.*** This is because pause just stops the kafka consumer fetch loop. Each iteration of the fetch loop can obtain a batch of messages (limited by `fetchMaxBytes`).\n\n### resume()\nResume the consumer. Resumes the fetch loop.\n\n### pauseTopics(topics)\nPause specify topics\n\n```\nconsumer.pauseTopics([\n    'topic1',\n    { topic: 'topic2', partition: 0 }\n]);\n```\n\n### resumeTopics(topics)\nResume specify topics\n\n```\nconsumer.resumeTopics([\n    'topic1',\n    { topic: 'topic2', partition: 0 }\n]);\n```\n\n### close(force, cb)\n* `force`: **Boolean**, if set to true, it forces the consumer to commit the current offset before closing, default `false`\n\nExample\n\n```js\nconsumer.close(true, cb);\nconsumer.close(cb); //force is disabled\n```\n\n## HighLevelConsumer\n### HighLevelConsumer(client, payloads, options)\n* `client`: client which keeps a connection with the Kafka server.\n* `payloads`: **Array**,array of `FetchRequest`, `FetchRequest` is a JSON object like:\n\n``` js\n{\n   topic: 'topicName'\n}\n```\n\n* `options`: options for consumer,\n\n```js\n{\n    // Consumer group id, deafult `kafka-node-group`\n    groupId: 'kafka-node-group',\n    // Optional consumer id, defaults to groupId + uuid\n    id: 'my-consumer-id',\n    // Auto commit config\n    autoCommit: true,\n    autoCommitIntervalMs: 5000,\n    // The max wait time is the maximum amount of time in milliseconds to block waiting if insufficient data is available at the time the request is issued, default 100ms\n    fetchMaxWaitMs: 100,\n    // This is the minimum number of bytes of messages that must be available to give a response, default 1 byte\n    fetchMinBytes: 1,\n    // The maximum bytes to include in the message set for this partition. This helps bound the size of the response.\n    fetchMaxBytes: 1024 * 1024,\n    // If set true, consumer will fetch message from the given offset in the payloads\n    fromOffset: false,\n    // If set to 'buffer', values will be returned as raw buffer objects.\n    encoding: 'utf8'\n}\n```\nExample:\n\n``` js\nvar kafka = require('kafka-node'),\n    HighLevelConsumer = kafka.HighLevelConsumer,\n    client = new kafka.Client(),\n    consumer = new HighLevelConsumer(\n        client,\n        [\n            { topic: 't' }, { topic: 't1' }\n        ],\n        {\n            groupId: 'my-group'\n        }\n    );\n```\n\n### on('message', onMessage);\nBy default, we will consume messages from the last committed offset of the current group\n\n* `onMessage`: **Function**, callback when new message comes\n\nExample:\n\n``` js\nconsumer.on('message', function (message) {\n    console.log(message);\n});\n```\n\n### on('error', function (err) {})\n\n\n### on('offsetOutOfRange', function (err) {})\n\n### addTopics(topics, cb, fromOffset)\nAdd topics to current consumer, if any topic to be added not exists, return error\n* `topics`: **Array**, array of topics to add\n* `cb`: **Function**,the callback\n* `fromOffset`: **Boolean**, if true, the consumer will fetch message from the specified offset, otherwise it will fetch message from the last commited offset of the topic.\n\nExample:\n\n``` js\nconsumer.addTopics(['t1', 't2'], function (err, added) {\n});\n\nor\n\nconsumer.addTopics([{ topic: 't1', offset: 10 }], function (err, added) {\n}, true);\n```\n\n### removeTopics(topics, cb)\n* `topics`: **Array**, array of topics to remove\n* `cb`: **Function**, the callback\n\nExample:\n\n``` js\nconsumer.removeTopics(['t1', 't2'], function (err, removed) {\n});\n```\n\n### commit(cb)\nCommit offset of the current topics manually, this method should be called when a consumer leaves\n\n* `cb`: **Function**, the callback\n\nExample:\n\n``` js\nconsumer.commit(function(err, data) {\n});\n```\n\n### setOffset(topic, partition, offset)\nSet offset of the given topic\n\n* `topic`: **String**\n\n* `partition`: **Number**\n\n* `offset`: **Number**\n\nExample:\n\n``` js\nconsumer.setOffset('topic', 0, 0);\n```\n\n### pause()\nPause the consumer. ***Calling `pause` does not automatically stop messages from being emitted.*** This is because pause just stops the kafka consumer fetch loop. Each iteration of the fetch loop can obtain a batch of messages (limited by `fetchMaxBytes`).\n\n### resume()\nResume the consumer. Resumes the fetch loop.\n\n### close(force, cb)\n* `force`: **Boolean**, if set to true, it forces the consumer to commit the current offset before closing, default `false`\n\nExample:\n\n```js\nconsumer.close(true, cb);\nconsumer.close(cb); //force is disabled\n```\n\n## Offset\n### Offset(client)\n* `client`: client which keeps a connection with the Kafka server.\n\n### events\n* `ready`: when zookeeper is ready\n* `connect` when broker is ready\n\n### fetch(payloads, cb)\nFetch the available offset of a specific topic-partition\n\n* `payloads`: **Array**,array of `OffsetRequest`, `OffsetRequest` is a JSON object like:\n\n``` js\n{\n   topic: 'topicName',\n   partition: 0, //default 0\n   // time:\n   // Used to ask for all messages before a certain time (ms), default Date.now(),\n   // Specify -1 to receive the latest offsets and -2 to receive the earliest available offset.\n   time: Date.now(),\n   maxNum: 1 //default 1\n}\n```\n\n* `cb`: *Function*, the callback\n\nExample\n\n```js\nvar kafka = require('kafka-node'),\n    client = new kafka.Client(),\n    offset = new kafka.Offset(client);\n    offset.fetch([\n        { topic: 't', partition: 0, time: Date.now(), maxNum: 1 }\n    ], function (err, data) {\n        // data\n        // { 't': { '0': [999] } }\n    });\n```\n\n### commit(groupId, payloads, cb)\n* `groupId`: consumer group\n* `payloads`: **Array**,array of `OffsetCommitRequest`, `OffsetCommitRequest` is a JSON object like:\n\n``` js\n{\n   topic: 'topicName',\n   partition: 0, //default 0\n   offset: 1,\n   metadata: 'm', //default 'm'\n}\n```\n\nExample\n\n```js\nvar kafka = require('kafka-node'),\n    client = new kafka.Client(),\n    offset = new kafka.Offset(client);\n    offset.commit('groupId', [\n        { topic: 't', partition: 0, offset: 10 }\n    ], function (err, data) {\n    });\n```\n\n### fetchCommits(groupid, payloads, cb)\nFetch the last committed offset in a topic of a specific consumer group\n\n* `groupId`: consumer group\n* `payloads`: **Array**,array of `OffsetFetchRequest`, `OffsetFetchRequest` is a JSON object like:\n\n``` js\n{\n   topic: 'topicName',\n   partition: 0 //default 0\n}\n```\n\nExample\n\n```js\nvar kafka = require('kafka-node'),\n    client = new kafka.Client(),\n    offset = new kafka.Offset(client);\n    offset.fetchCommits('groupId', [\n        { topic: 't', partition: 0 }\n    ], function (err, data) {\n    });\n```\n\n### fetchLatestOffsets(topics, cb)\n\nExample\n\n```js\n\tvar partition = 0;\n\tvar topic = 't';\n\toffset.fetchLatestOffsets([topic], function (error, offsets) {\n\t\tif (error)\n\t\t\treturn handleError(error);\n\t\tconsole.log(offsets[topic][partition]);\n\t});\n```\n\n\n# Troubleshooting / FAQ\n\n## HighLevelProducer with KeyedPartitioner errors on first send\n\nError:\n\n```\nBrokerNotAvailableError: Could not find the leader\n```\n\nCall `client.refreshMetadata()` before sending the first message. Reference issue [#354](https://github.com/SOHU-Co/kafka-node/issues/354)\n\n\n\n## How do I debug an issue?\nThis module uses the [debug module](https://github.com/visionmedia/debug) so you can just run below before starting your app.\n\n```bash\nexport DEBUG=kafka-node:*\n```\n\n\n## For a new consumer how do I start consuming from the latest message in a partition?\n\n1. Call `offset.fetchLatestOffsets` to get fetch the latest offset\n2. Consume from returned offset\n\nReference issue [#342](https://github.com/SOHU-Co/kafka-node/issues/342)\n\n\n## FailedToRebalanceConsumerError: Exception: NODE_EXISTS[-110]\n\nThis error can occur when a HLC is killed and restarted quickly. The ephemeral nodes linked to the previous session are not relinquished in zookeeper when `SIGINT` is sent and instead relinquished when zookeeper session timeout is reached. The timeout can be adjusted using the `sessionTimeout` zookeeper option when the `Client` is created (the default is 30000ms).\n\nExample handler:\n\n```js\nprocess.on('SIGINT', function () {\n    highLevelConsumer.close(true, function () {\n        process.exit();\n    });\n});\n```\n\nAlternatively, you to avoid this issue entirely by omitting the HLC's `id` and a unique one will be generated for you.\n\nReference issue [#90](https://github.com/SOHU-Co/kafka-node/issues/90)\n\n## HighLevelConsumer does not consume on all partitions\n\nYour partition will be stuck if the `fetchMaxBytes` is smaller than the message produced.  Increase `fetchMaxBytes` value should resolve this issue.\n\nReference to issue [#339](https://github.com/SOHU-Co/kafka-node/issues/339) \n\n## How to throttle messages / control the concurrency of processing messages\n\n1. Create a `async.queue` with message processor and concurrency of one (the message processor itself is wrapped with `setImmediate` so it will not freeze up the event loop)\n2. Set the `queue.drain` to resume the consumer\n3. The handler for consumer's `message` event pauses the consumer and pushes the message to the queue.\n\n# Running Tests\n\n### Install Docker\n\nOn the Mac you can either install `docker-machine` or [Docker for Mac](https://docs.docker.com/engine/installation/mac/).\n\nDocker machine:\n\n```bash\nbrew install docker docker-machine docker-compose\ndocker-machine create --driver virtualbox dev\n```\n\n### Start Docker and Run Tests\n\n```bash\nnpm test\n```\n\n### Stop Docker\n\n```bash\nnpm run stopDocker\n```\n\n\n# LICENSE - \"MIT\"\nCopyright (c) 2015 Sohu.com\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\nof the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n",
  "readmeFilename": "README.md",
  "homepage": "https://github.com/SOHU-Co/kafka-node",
  "_id": "kafka-node@0.5.7",
  "_from": "kafka-node@"
}
